---
title: "Regression Models Course Project"
author: "kosw"
date: "2023-11-25"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

In this report, we explored the regression model with car's mpg(Miles/gallon) as reponse variable using 'mtcars' dataset. We excluded several variables among 10 variables(except mpg, response variable) that mtcars dataset contains, using correlation between variables and vif. After that, we diagnosed assumptions about errors with some diagnostic plots. Conclusionally, we got significant multivariate regression model explaining mpg variable with 3 variables : am, vs, carb (also with intercept)

## Load Data

First, Load some packages we will use in this research : ggplot2, GGally, car
```{r, results='hide', message=FALSE, echo=FALSE}
library(ggplot2)
library(GGally)
library(car)
```

And Load Dataset we will use : mtcars
```{r}
x <- mtcars
```

## Exploratory Data Analysis

This dataset comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973â€“74 models).
And using this data, we wanna make regression model that has 'mpg' variable as reponse. And the most interested variable as regressor is 'am'.
For exploration of the relationship between all variables briefly, let's make an pair plot!(see plot1)


## First Model

For our first regression model, we will exclude one of two variables that has correlation over 0.8.
It is because two variables that has strong correlation may explain reponse variable similarly, and including
both variable can decrease the estimation accuracy of each others.
When two variable has correlation over 0.8, we will remove one that has less correlation with 'am' variable because
'am' variable is variable we interested in.
In this way, here we will exclude 2 variables : disp, cyl

Then we can make first model including all of the rest of variables.
```{r}
model1 <- lm(mpg ~ am+hp+drat+wt+qsec+vs+gear+carb, x)
```

```{r, echo=FALSE}
summary(model1)$coef[,4]
```

Above are p-values of each regressors.
In this model, only one variable among 9 is siginificant - surely problem!!! 
It is presumably because, we include too many variables, so they decrease the accuracy of estimation.
So We need to exclude a few more variables.
Which variable will we exclude?
To get significant conclusion, we will exclude variable that has the strongest correlation with other variables.
We can see it through vif.
```{r, echo=FALSE}
vif(model1)
```
Let's exclude the largest of vif : wt


## Second Model

By excluding 'wt' variable, we can make our second model.
```{r}
model2 <- lm(mpg ~ am+hp+drat+qsec+vs+gear+carb, x)
```


```{r, echo=FALSE}
summary(model2)$coef[,4]
```

And, also not all variables are significant.
So, by repeating this method until all variables are signigicant, we get out final model.
```{r, echo=FALSE}
vif(model2)
```

</br></br>

## Final Model

Final model is following.
```{r}
model3 <- lm(mpg ~ am+vs+carb, x)
```


```{r, echo=FALSE}
summary(model3)$coef
```

In this model, all regressors are significant.

Let's try anova to check each variables are significant once more compared with several nested models.

```{r, echo=FALSE}
model31 <- lm(mpg ~ am, x)
model32 <- lm(mpg ~ am+vs, x)

anova(model31, model32, model3)
```
All coefficient are significant so, It seems better to use our final model.

Our model's coefficient's 95 percent confidence interval is following.
```{r, echo=FALSE}
confint(model3)
```



## Residual Plot and Diagnostics

Let's make some diagnostic plot including residual plot and check several things about residuals and data points.


First, from residual plot(see plot2),it seems that the residuals have don't have specific pattern.
It meets the assumption of the linear model.


And from residual Q-Q plot(see plot3), it seems that the distrbution of residuals are almost normal.

```{r, echo=FALSE}
shapiro.test(resid(model3))
```
For more accurate test, we can do shapiro.test.
In shapiro.test, p-value is over 0.05, so we fail to reject the null hypothesis : this distribution is normally distributted.
So we can say our residuals are normal.



From Residual vs Leverage plot(see plot4), The largest leverage point has residual near zero. It conforms to regression line. And there is no point that has cook's distance over 1. So there is no particularly influential point.

## Conclusion

From final model, we can say 4 thing from our conclusion of 4 coefficients.

* First, all regressor are zero('automatic'/'V-shaped Engine'/'0 carburetors), the expected value of mpg is 19.5174 Miles/gallon
* Second, When our am variable(Transmisson) goes automatic to manual holding other variables constant,the expected change in mpg value is increase 6.7980 Miles/gallon
* Third, When our vs variable(Engine) goes 'V-shaped' to 'straight' holding other variables constant, the expected change in mpg value is increase 4.1957 Miles/gallon.
* Finally, One more carburetor holding other variables constant, the expected change in mpg value is decrease -1.4308 Miles/gallon.

Additionally, And our model's R Squared value is '', which means that our regression model is explaining significant proportion of the Total variance of observed data.

About the following 2 questions of interest : "Is an automatic or manual transmission better for MPG?", "Quantify the MPG difference between automatic and manual transmissions", we can say that manual transmission is 6.7980 Miles/gallon better for mpg than automatic when holding other variables constant.

</br></br>

## Appendix

```{r, echo=FALSE}
ggpairs(x)
```

(plot1)

```{r, echo=FALSE}
plot(model3, 1)
```

(plot2)

```{r, echo=FALSE}
plot(model3, 2)
```

(plot3)

```{r, echo=FALSE}
plot(model3, 5)
```

(plot4)